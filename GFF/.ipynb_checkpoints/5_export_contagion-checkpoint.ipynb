{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import community \n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from seaborn import color_palette, set_style, palplot\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    keep_var = ['countrycode','counterpart_code','country','counterpart','year',\n",
    "                'CDIS_IAD','CPIS_IAP','CDIS_IADE','CDIS_IADD','CPIS_IAPE','CPIS_IAPD','loans_dep']\n",
    "    df = df[keep_var]                           ## keep only used variables \n",
    "    df = df.replace(np.nan,0)                   ## turn na to zero \n",
    "    num = df._get_numeric_data()\n",
    "    num[num < 0] = 0                            ## turn negative to zero \n",
    "    df['total'] = df[['CDIS_IAD','CPIS_IAP','loans_dep']].sum(axis=1)\n",
    "\n",
    "    mata = ['countrycode','counterpart_code','country','counterpart','year']\n",
    "    var_org = ['CDIS_IADE','CDIS_IADD','CPIS_IAPE','CPIS_IAPD','loans_dep','total']\n",
    "    var_sum_out = [e+'_Sum_out' for e in var_org]\n",
    "    var_sum_in = [e+'_Sum_in' for e in var_org]\n",
    "    var_weight = [e+'_weight' for e in var_org]\n",
    "\n",
    "    df[var_sum_out]= df.groupby(['countrycode','year'])[var_org].transform(sum)           ## like stata egen sum \n",
    "    df[var_sum_in]= df.groupby(['counterpart_code','year'])[var_org].transform(sum)        ## like stata egen sum \n",
    "    df_weight = pd.DataFrame((df[var_org].values / df[var_sum_out].values)*100,columns=[var_weight])\n",
    "    df[var_weight] = df_weight                                                        ## create the weight variables \n",
    "    mata.extend(var_weight)\n",
    "    df = df[mata]\n",
    "    df.fillna(0,inplace=True)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def get_nx_community(G,var):\n",
    "#algorism: https://sites.google.com/site/findcommunities/\n",
    "#package: http://perso.crans.org/aynaud/communities/\n",
    "    \n",
    "    ## use adj matrix + its invert, so the edge will be the sum of in and out edge weight \n",
    "    node_list = G.nodes()\n",
    "    node_list.sort()\n",
    "    A = nx.to_numpy_matrix(G = G,nodelist=node_list,weight=var)\n",
    "    ud_M = A + A.T \n",
    "    ud_G = nx.from_numpy_matrix(ud_M)\n",
    "    ## relable node to country name \n",
    "    maplist = dict(zip(ud_G.nodes(), node_list))\n",
    "    ud_G = nx.relabel_nodes(ud_G,maplist) \n",
    "    l_community = community.best_partition(ud_G,weight='weight',resolution=1)\n",
    "    nx.set_node_attributes(G, 'nx_community', l_community)\n",
    "    \n",
    "def get_eigen_centrality(G,var):\n",
    "        ## eigenvector centrality\n",
    "    e = nx.eigenvector_centrality_numpy(G,weight=var)\n",
    "    nx.set_node_attributes(G, 'eigenvector_centrality', e) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge contaigion data go G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cont_data(file,sheet):\n",
    "    df_c = pd.read_excel(file,sheet)\n",
    "    time_steps = df_c['time step'].unique()\n",
    "    for sp in time_steps:\n",
    "        df_c['step'+str(sp)] = df_c['time step'].isin(list(range(sp+1))).astype(int)\n",
    "    \n",
    "    return df_c\n",
    "\n",
    "def load_cont_single_layer_data(df_cm,layer):\n",
    "    df_c = df_cm[df_cm[' layer']==layer]\n",
    "    time_steps = df_c['time step'].unique()\n",
    "    for sp in time_steps:\n",
    "        df_c['step'+str(sp)] = df_c['time step'].isin(list(range(sp+1))).astype(int)\n",
    "    \n",
    "    return df_c\n",
    "\n",
    "def create_steps(G,df_c,time_steps):\n",
    "    G_nodes= pd.DataFrame(G.nodes(),columns=['country'])\n",
    "    var_list = ['step'+str(v) for v in time_steps]\n",
    "    var_list.append('country')\n",
    "    merge_c=df_c[var_list]\n",
    "    G_merged = pd.merge(G_nodes,merge_c,on='country',how='left')\n",
    "    var_list.remove('country')\n",
    "    G_merged.loc[G_merged.country=='United States',var_list] = 1\n",
    "    G_merged.fillna(0,inplace=True)\n",
    "\n",
    "    return G_merged\n",
    "\n",
    "def merge_steps(G,G_merged,time_steps):\n",
    "    steps = ['step'+str(v) for v in time_steps]\n",
    "    for s in steps:\n",
    "        con_dict = dict(zip(G_merged['country'],G_merged[s]))\n",
    "        con_dict = {key: int(value) for (key,value) in con_dict.items()}\n",
    "        nx.set_node_attributes(G, s, con_dict)\n",
    "\n",
    "######################\n",
    "## run all processes##\n",
    "######################\n",
    "\n",
    "def get_contagion_data(G,file,sheet):\n",
    "    df_c = load_cont_data(file,sheet)\n",
    "    time_steps = df_c['time step'].unique()\n",
    "    G_merged = create_steps(G,df_c,time_steps)\n",
    "    merge_steps(G,G_merged,time_steps)\n",
    "    \n",
    "def get_contagion_single_layer(G,df_cm,layer):\n",
    "    df_c = load_cont_single_layer_data(df_cm,layer)\n",
    "    time_steps = df_c['time step'].unique()\n",
    "    G_merged = create_steps(G,df_c,time_steps)\n",
    "    merge_steps(G,G_merged,time_steps)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### export the entire process##\n",
    "###############################\n",
    "\n",
    "def export_gephi(df,year,var,file,sheet):\n",
    "    ## clean the data first \n",
    "    df_y = df[df['year']==year]\n",
    "    df_y.fillna(0,inplace=True)\n",
    "    df_y = df_y[df_y[var]>0]\n",
    "    G = nx.from_pandas_dataframe(df_y, source=\"country\", target=\"counterpart\", edge_attr=[var],create_using=nx.DiGraph())\n",
    "    #get_hierarchy_cluster(G,var)                                   ## add hierarchy_cluster to node attribute\n",
    "    get_nx_community(G,var)                                        ## add nx community detection to node attribute\n",
    "    get_eigen_centrality(G,var)\n",
    "    get_contagion_data(G,file,sheet) ## merge contagion data to G \n",
    "    \n",
    "    nx.write_gexf(G, \"../result/gexf/\"+var+str(year)+\".gexf\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "def export_single_layer_contagion(df,year,var,df_cm,layer):\n",
    "    ## clean the data first \n",
    "    df_y = df[df['year']==year]\n",
    "    df_y.fillna(0,inplace=True)\n",
    "    df_y = df_y[df_y[var]>0]\n",
    "    G = nx.from_pandas_dataframe(df_y, source=\"country\", target=\"counterpart\", edge_attr=[var],create_using=nx.DiGraph())\n",
    "    #get_hierarchy_cluster(G,var)                                   ## add hierarchy_cluster to node attribute\n",
    "    #get_nx_community(G,var)                                        ## add nx community detection to node attribute\n",
    "    get_eigen_centrality(G,var)\n",
    "    get_contagion_single_layer(G,df_cm,layer) ## merge contagion data to G \n",
    "    \n",
    "    nx.write_gexf(G, \"../result/gexf/\"+'multi_layer_'+var+str(year)+\".gexf\")\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read pre-processed data from stata\n",
    "df = pd.read_stata('../data/0_CPIS_CDIS_BIS_USTIC_merged_fixed1.dta')\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['countrycode', 'counterpart_code', 'country', 'counterpart', 'year',\n",
       "       'CDIS_IADE_weight', 'CDIS_IADD_weight', 'CPIS_IAPE_weight',\n",
       "       'CPIS_IAPD_weight', 'loans_dep_weight', 'total_weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export aggregated layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## export aggregate layer for 2015 shock on US\n",
    "G = export_gephi(df,2015,'total_weight','../result/contagion/country_match.xlsx','Agg_US_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eigenvector_centrality': 0.7338163250298024,\n",
       " 'nx_community': 3,\n",
       " 'step0': 1,\n",
       " 'step1': 1,\n",
       " 'step2': 1,\n",
       " 'step3': 1,\n",
       " 'step4': 1,\n",
       " 'step5': 1,\n",
       " 'step6': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node['United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Export multi layer one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_map = {0:'CDIS_IADE_weight',1:'CDIS_IADD_weight',2:'CPIS_IAPE_weight',3:'CPIS_IAPD_weight',4:'loans_dep_weight'}\n",
    "df_cm = pd.read_excel('../result/contagion/country_match.xlsx','Multi_US_09')    # pass in file name and sheet name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key,value in layer_map.items():\n",
    "    print((key,value))\n",
    "    G = export_single_layer_contagion(df,2009,value,df_cm,key)\n",
    "    print(G.node['United States'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
